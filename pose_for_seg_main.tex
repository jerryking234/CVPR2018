\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}       % blackboard math symbols
%\usepackage{nicefrac}       % compact symbols for 1/2, etc.
%\usepackage{microtype}      % microtypography

\usepackage{url}
\usepackage[table]{xcolor}
\usepackage{bbm}
\usepackage{booktabs}
\usepackage[T1]{fontenc}
\usepackage{fix-cm}
\usepackage{array}
\usepackage{epsfig}
%\usepackage{mathabx}
%\usepackage{dsfont}
\usepackage{multirow}

\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{color}
\usepackage{epstopdf}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{calc}
\usepackage{multirow}
\usepackage{xspace}
\usepackage{booktabs}
\usepackage{mathrsfs}
\usepackage{array}
\usepackage{gensymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \cvprfinalcopy % *** Uncomment this line for the final submission
\newcommand{\figref}[1]{Fig\onedot~\ref{#1}}
\newcommand{\equref}[1]{Eq\onedot~\eqref{#1}}
\newcommand{\secref}[1]{Sec\onedot~\ref{#1}}
\newcommand{\tabref}[1]{Tab\onedot~\ref{#1}}
\newcommand{\thmref}[1]{Theorem~\ref{#1}}
\newcommand{\prgref}[1]{Program~\ref{#1}}
\newcommand{\algref}[1]{Alg\onedot~\ref{#1}}
\newcommand{\clmref}[1]{Claim~\ref{#1}}
\newcommand{\lemref}[1]{Lemma~\ref{#1}}
\newcommand{\ptyref}[1]{Property\onedot~\ref{#1}}

\newcommand{\ve}[1]{{\mathbf #1}} % for displaying a vector or matrix
\newcommand{\hua}[1]{{\mathcal #1}}
\newcommand{\scr}[1]{{\mathcal #1}}
\newcommand{\by}[2]{\ensuremath{#1 \! \times \! #2}}

%\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\onedot{\ifx\@let@token.\else.\null\fi\xspace}
\def\eg{\emph{e.g.}}
\def\Eg{\emph{E.g}\onedot}
\def\any{\forall}
\def\ie{\emph{i.e.}}
\def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{cf}\onedot}
\def\Cf{\emph{Cf}\onedot}
\def\etc{\emph{etc}\onedot}
\def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot}
\def\dof{d.o.f\onedot}
\def\etal{\emph{et al.}}

\def\cvprPaperID{936} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{DeepLocSeg: Online Camera Pose Estimation and Scene Parsing with Deep Learning and a 3D Semantic Map}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
Visual-based outdoor navigation requires accurately localizing the camera and preferably per-pixel semantic understanding, which can be widely applied for autonomous driving, or augment reality \etc.
%However, system solely relying on visual signal is non-robust due to visual confusion across multiple scenes.
%Thus, coarse signals from motion sensors, \eg GPS and IMU, are usually considered as a localization prior~\cite{}.
In this paper, we propose a deep learning based method for localizing the camera and parsing the recorded video simultaneously in a single framework, which fuses signals from camera and motion sensors like GPU and IOU.
Specifically in our setting, we have a 3D semantic world, and a video is recording online inside, meanwhile a non-accurate camera pose signal is observed.
In our system, at each timestamp, based on the obtained pose signal, we render a label map out of the 3D world, and feed it to a pose CNN jointly with the current frame of image, yielding a corrected camera pose.
Then, a multi-layer recurrent neural network (RNN) is performed afterwards, which captures high-order temporal information that further improve the pose accuracy.
Finally, based on the corrected pose from RNN, a new label map is rendered, and is fed into a segment CNN combining with the image, yielding a parsing of the scene.
In order to perform the experiments,  we build a dataset with a semantic labeled real world 3D map, jointly with many recorded videos with ground truth poses from high accurate motion sensors.
We show that practically, pose estimation solely relying on images like PoseNet~\cite{Kendall_2015_ICCV} may fail due to street view confusion, and it is important to fuse multi sensors. In addition, semantic parsing and pose estimation are mutually beneficial in learning more robust networks. Finally, various ablation studies are performed, which demonstrate the effectiveness of the proposed system.
\end{abstract}

%%%%%%%%% BODY TEXT
\input{intro}
\input{related}
\input{dataset}
\input{algorithm}
\input{exp}


\section{Conclusion}
\label{sec:conclusion}
In this paper, we present a unified strategy for performing video localization and segmentation with deep CNN and a 3D semantic map, for the applications of augmented reality navigation. The algorithm is simple and run efficient while yielding strong results in parsing backgrounds. 

\paragraph{Future work.} In terms of dataset, additional to a relative small data with few objects used in this paper, yielding limited segment model for object, significantly larger and harder data will be collected in the near future.
For algorithm, we would like to simplify the 3D point cloud map with 3D models, based on which a better and faster renderring can be performed.
Moreover, we can follow the LSD-SLAM~\cite{engel2014lsd} to further re-localize the camera through perpixel semantic matching, which could potentially further improve the pose accuracy. 

%
{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}

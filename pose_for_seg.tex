\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{url}
\usepackage[table]{xcolor}
\usepackage{bbm}
\usepackage{booktabs}
\usepackage[T1]{fontenc}
\usepackage{fix-cm}
\usepackage{array}
\usepackage{epsfig}
%\usepackage{mathabx}
\usepackage{dsfont}
\usepackage{multirow}

\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{color}
\usepackage{epstopdf}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{calc}
\usepackage{multirow}
\usepackage{xspace}
\usepackage{booktabs}
\usepackage{mathrsfs}
\usepackage{array}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \cvprfinalcopy % *** Uncomment this line for the final submission
\newcommand{\figref}[1]{Fig\onedot~\ref{#1}}
\newcommand{\equref}[1]{Eq\onedot~\eqref{#1}}
\newcommand{\secref}[1]{Sec\onedot~\ref{#1}}
\newcommand{\tabref}[1]{Tab\onedot~\ref{#1}}
\newcommand{\thmref}[1]{Theorem~\ref{#1}}
\newcommand{\prgref}[1]{Program~\ref{#1}}
\newcommand{\algref}[1]{Alg\onedot~\ref{#1}}
\newcommand{\clmref}[1]{Claim~\ref{#1}}
\newcommand{\lemref}[1]{Lemma~\ref{#1}}
\newcommand{\ptyref}[1]{Property\onedot~\ref{#1}}
\newcommand{\ve}[1]{{\mathbf #1}} % for displaying a vector or matrix
\newcommand{\hua}[1]{{\mathcal #1}}
\newcommand{\scr}[1]{{\mathcal #1}}
\newcommand{\by}[2]{\ensuremath{#1 \! \times \! #2}}
\newcommand{\thickhline}{%
    \noalign {\ifnum 0=`}\fi \hrule height 1pt
    \futurelet \reserved@a \@xhline
}

%\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\onedot{\ifx\@let@token.\else.\null\fi\xspace}
\def\eg{\emph{e.g.}} 
\def\Eg{\emph{E.g}\onedot}
\def\any{\forall}
\def\ie{\emph{i.e.}} 
\def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{cf}\onedot} 
\def\Cf{\emph{Cf}\onedot}
\def\etc{\emph{etc}\onedot} 
\def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} 
\def\dof{d.o.f\onedot}
\def\etal{\emph{et al.}}

\def\cvprPaperID{936} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{DeepLocSeg: Camera Pose Estimation and Scene Parsing with 3D Semantic World and Deep CNN}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
Visual-based outdoor navigation requires accurately localizing the camera and preferably per-pixel semantic understanding. It can be widely applied for autonomous driving, or augment reality navigation \etc.
%However, system solely relying on visual signal is non-robust due to visual confusion across multiple scenes. 
%Thus, coarse signals from motion sensors, \eg GPS and IMU, are usually considered as a localization prior~\cite{}.
In this paper, we propose a deep learning based method for localizing the camera and parsing the recorded video simultaneously in a single framework, which fuses signals from camera and motion sensors like GPU and IOU.
Specifically, assuming we have a 3D semantic world, and the testing video is recording inside. Rough pose signal is also obtained online. In our system, for each frame in a moving camera, based on the obtained coarse pose signal, we render a label map out from our 3D world online, and feed it to a pose CNN jointly with the current frame of image, yielding a corrected pose. 
Then, a multi-layer recurrent neural network (RNN) is performed afterwards in order to capture high-order temporal information. 
Finally, a new label map is rendered again based on the corrected pose, which is feed into a segment CNN combining with the image.
In order to perform the experiments,  we built a dataset with a semantic labeled real world 3D map, jointly with many recorded videos with ground truth pose from high accurate motion sensors. We show that firstly, in a relative large environment, unlike the PoseNet~\cite{}, it is important to have a reference coarse pose signal. In addition, semantic information and pose are mutually beneficial in learning more robust networks. Finally, various ablation studies are performed, which demonstrate the effectiveness of the proposed system.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:introduction}
% the problem we are solving, distinguish with previous problems
In the applications like car navigation, due to the inaccuracy from motion sensors from mobile GPS and IMU, outdoor visual-based 6-DOF camera pose estimation and scene parsing~\cite{} are attracting much attention in the community of computer vision. 
Additionally, to acquire better scene understanding for applications like augment reality navigation~\cite{}, parsing each frame of a video is also important.

% existing methods only consider one of the tasks with soly visual signal. 
Currently, most existing vision algorithms are trying to solve both of the tasks solely depend on visual signals. 
For instances, in camera localization, geometric based methods are relying visual feature tracking, \eg systems of SLAM~\cite{}, PTAM~\cite{} and 
DTAM~\cite{}, where the environment point cloud is reconstructed during the localization. Such systems could be confused of the environment scale and also can not work when staic videos or a single image is provided.
To handle such senarios, visual feature matching or point cloud matching are oftenly used, \eg system of Perspective-n-Points (PnP)~\cite{}, or patch matching~\cite{}. 
Most recently, deep learning based methods, \eg either for images~\cite{} or videos~\cite{}, are also proposed for real-time localization, which show superior accuracy and speed tradeoff.
Nevertheless, single view methods are especially for specified feature rich landmarks, \eg, Cambridge buidings~\cite{}, while could easily fail for street views with very similar appearances.

For scene parsing, approaches~\cite{} based on deep fully convolutional network (FCN)~\cite{} and ResNet~\cite{} are the best algorithms for either single image or multiple frames according to Dataset like CityScape~\cite{}. For video inputs, recent works~\cite{} also try to incorporate the optical flow between consecutive frames, in order to accelerate the parsing and build the consistency along the temperal dimension.
However, since the camera is recording the same 3D scene, higher order relationship beyond nearby frame should also be considered.
In order to jointly consider parsing and 3D, prior arts~\cite{} before deep-learning try to do it typically by first do the reconstruction with structure-from-motion (SFM)~\cite{} and then parsing the images, 
 whereas the accuracy of reconstruted 3D from street-vew images is limited for real applications.

In our scenario, targeting at a more practical system setting, in addition to online recorded video, for handling the localization confusion of street views, we propose to fuse the coarse GPS and IMU motion signal from corresponding device, which is typically available for current navigation system. Those signals can serve as a pose priori for our deep learning system. 
For video segmentation, in order to have 3D relationships between different frames, we pre-build a high resolution semantic 3D map based on high accurate LIDAR points recorded by Riegl~\cite{}, which helps construct the correlation in-between multiple frames through the estimated poses. 
In fact, our setting is realistic when compare with the widely applied commercial navigation system, such as Google Map~\cite{}, we raise the 2D road map to a 3D world, and try to online estimate the 3D camera pose rather than 2D. For 3D map, city scale 3D map has largely already collected from comany like Google Earth~\cite{} and Altzure~\cite{}, and also semantic labeled ones is also built such as Toronto city~\cite{}.

Finally, within such a framework, the pose estimation and scene parsing could be jointly modeled, where pose can help build the correlation between different frames, and reversely, semantics could help align camera poses, yielding better results for both tasks. We will show such benefits in our experiments.

% redefine our problem
In summary, the contributions of this paper are in three folds:
\begin{itemize}
    \item We propose a deep learning based method for fusing multiple sensors, \ie camera, GPS and IMU, which significantly improves the accuracy of awared visual tasks, \ie camera localization and scene parsing.
    \item We build a system that jointly do 6-DOF camera pose estimate and semantic parsing based on a deep learning system. where two kinds of information are mutually benefitial in the system.
    \item To perform the experiments, we apply our system to a dataset recorded from real scene with dense 3D point cloud, ground truth camera poses and pixel-level parsing of every frame, which helps to evaluate the system and hopefully benefit the correlated research field in computer vision.
\end{itemize}

The structure of this paper is organized as follows. In \secref{sec:data_collection}, we first describe how our data is different from the existing outdoor datasets. In addition, we briefly introduce the method how the data is collect and labelled. Then, \secref{sec:localize_and_parsing} presents the framework and detail of our system. The full performance is evaluated quantitatively for both pose estimation and parsing in \secref{sec:experiments}, and \secref{sec:conclusion} concludes the paper and point out future directions. Finally, We will release all our code, models and dataset with the publication of this paper.

\begin{figure*}
\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
   \caption{Framework of our proposed system. The black arrows show the testing process and red arrows show the back-propagation in training.}
\label{fig:framework}
\end{figure*}

\subsection{Framework}
\label{sub:framework}
The framework of our system is illustrated in \figref{fig:framework}. At above, a pre-built 3D semantic map is prepared. During testing, an online stream of images and corresponding camera poses are feed in to the system. Firstly, for each frame, a semantic label map is rendered out based on the obtained coarse camera pose, and is fed jointly with the image to a pose network,  which calculates their spacial relative rotation and translation, and yields a corrected camera pose. Then, the corrected poses are fed into a RNN with GRU that further improve the pose accuracy by capturing the temporal information in the stream. 
Last, given the rectified camera pose, a new rendered label map is generated, and fed in a segmentation network together with the image, which helps to segment a spacial more accuracy and temporal more consistent segmentation results.
In this system, since our data contains ground truth for both pose and segments, thus it can be trained with strong supervision at each end of outputs.

\section{Related work}
\label{sec:related_work}

% traditional key points, slam etc
% pnp
% marc's paper
% hongdong's paper
% posenet related
% iccv kalman 
% domain transfer etc


\section{Build the data}
\label{sec:data_collection}

\paragraph{Motivation.}
As described in the \secref{sub:framework}, our system is performed with available motion sensors and a semantic 3D map. 
However, similar outdoor dataset produced by prior arts such as KITTI and CityScapes, does not containing such information. As summarized in \tabref{tbl:data}, we list several key properties to perform our experiments. 
As can be seen, existing ones do not satisfy our requirements, especially for pose estimation, for training, we want the recorded video to roughly cover the 3D environment, so that when new images come in, appearance similar views had been seen during training. This means repeated recording at similar spacial locations are required in the data, which we call temporal variations in \tabref{tbl:data}.
However, most existing datasets are a temporal snapshot at some locations in the environment, which require us to collect a new dataset ourselves.

\paragraph{Data collection.}


\paragraph{Data labelling.}


\begin{table*}[t]
\center
\begin{tabular}{lccccc}
\toprule[0.2 em]
% \thickhline
Dataset & Real Data & Camera pose & 3D map & Video per-frame labeling  & Temporal variations  \\ 
\hline 
\multicolumn{1}{l|}{CamVid~\cite{}}     &\checkmark                       & -              & -              &  -  & -  \\
\multicolumn{1}{l|}{KITTI~\cite{}}      &\checkmark  & \checkmark     & sparse points  & -   & -  \\
\multicolumn{1}{l|}{CityScapes~\cite{}} &\checkmark  & -              &  -             & selected frames & - \\
\multicolumn{1}{l|}{Toronto~\cite{}}    &\checkmark  & \checkmark     & sparse points  & selected pixels & - \\
\hline
\multicolumn{1}{l|}{Synthia~\cite{}}    & -          & \checkmark     & -       &\checkmark & -    \\ 
\multicolumn{1}{l|}{Play for benchmark~\cite{}} &-   & \checkmark     & -     &\checkmark & - \\
\hline 
\multicolumn{1}{l|}{Ours}              & Real        &\checkmark    &dense point cloud  & \checkmark     &  \checkmark \\
\toprule[0.2 em]
\end{tabular}
\caption{Compare our data with the other related outdoor street-view datasets for our task. 'Real Data' mean whether the data is collected from realistic world. 
'3D map' means whether it contains 3D map of the whole dataset. 'Video per-frame labeling' means whether it has per-frame per-pixel semantic label. 
'Temporal variations' mean whether the recorded video can roughly cover the whole scene, but have multiple.}
\label{tbl:data}
\vspace{-0.3\baselineskip}
\end{table*}


\begin{figure}[t]
\begin{center}
\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
\end{center}
   \caption{Compare the rendered results with and without point enlargement handled by \equref{eq:label}.}
\label{fig:render}
\end{figure}

\section{Localizing camera and scene parsing.}
\label{sec:localize_and_parsing}
As shown in \secref{sub:framework}, our full system is based on a semantic 3D map and deep CNN. In the following, we will first describe how a semantic label map is rendered from the 3D, then talk about the details of our network architecture and the loss functionals to train the system.

\paragraph{Render a label map from a camera pose.} 
Formally, given a 6-DOF camera pose $\ve{p} = [\ve{q}, \ve{t}]$, where $\ve{q}$ is the quaternion representation of rotation and $\ve{t}$ is translation, a label map can be rendered from the semantic 3D map, where z-buffer~\cite{} is applied to find the closest point at each pixel.

In our setting, the 3D map is a point cloud based environment which includes 200M points. Although the density of the point cloud is very high (one point per 2.5 centimeters within road regions), when the 3D points are far away from the camera, the projected points could be very sparse, \eg regions of buildings or advisory board etc.
Thus for each point in the environment, we enlarge the 3D point to a square where the square size is dependent on its semantic class. Specifically, for each 3D point belong a class, the square size is set based on the class's average distance to the camera, 
\begin{align}
\label{eq:label}
s_c = \alpha N(d(\hua{P}_c, \hua{T}), [1, 2])
\end{align}
where

With such a strategy, as shown by \figref{fig:render}, many missing regions in-between projected point are in-painted, while the boundaries between different label segments are much clearer. By feeding in such a rendered map without lots of invalid values, it facilitates the convolutional kernels of CNN to better discover scene layouts.
One may wonder why we render semantic label map rather than synthesizing an RGB image. This is because label maps is invariant to color changes, which yields better boundaries reflecting the scene layout.

\subsection{Camera Localization with motion prior}

\paragraph{Pose network architecture.} 
Given the current coarse pose $\ve{p}_{i}$ from GPS and IMU, we first render a label map, concatenated with the inputs image. 


\paragraph{Semantic weighted pose loss.} 
Following the PoseNet~\cite{}, we use the geometric loss for training, which avoid the balancing factor between rotation and translation. 
Formally, for each frame, given a set of current visible  the loss can be written as,
% \begin{align}

% \end{align}
where $\lambda_e$ and $\lambda_g$ are balancing parameters
Additionally, since the amount of points in each class is dramatically unbalanced,


\subsection{Video parsing with pose guidance}


\paragraph{Segment network architecture.}


\section{Experiments}
\label{sec:experiments}


\begin{figure}[t]
\begin{center}
\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
\end{center}
   \caption{Example of caption.  It is set in Roman so that mathematics
   (always set in Roman: $B \sin A = A \sin B$) may be included without an
   ugly clash.}
\label{fig:long}
\label{fig:onecol}
\end{figure}

\paragraph{Implementation details.} For rendering from 3D map, we adopt OpenGL to efficiently render a label map while handling the z-buffer operation. A 512 $\times$ 608 image can be generated in 70ms with a Titan Z GPU. 

\subsection{Ablation study.}

\paragraph{Pose Evaluation.}

\paragraph{Segment Evaluation.}

\paragraph{Qualitative results.}

\paragraph{Discussion.}

\paragraph{Future work.} Since theoretically there exists optimal solutions which enforce the consistency between rendered label map from a camera pose and corresponding semantic segments, 
later we would like to design a real time solution for finding it for better localization.
In addition, we would like to model the surfaces which sketch out the 3D map, depend on our point cloud data, based on which a simpler and faster dense segment would performed.


\section{Conclusion}
\label{sec:conclusion}

% 
{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}

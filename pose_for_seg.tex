\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{url}
\usepackage[table]{xcolor}
\usepackage{bbm}
\usepackage{booktabs}
\usepackage[T1]{fontenc}
\usepackage{fix-cm}
\usepackage{array}
\usepackage{epsfig}
%\usepackage{mathabx}
\usepackage{dsfont}
\usepackage{multirow}

\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{color}
\usepackage{epstopdf}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{calc}
\usepackage{multirow}
\usepackage{xspace}
\usepackage{booktabs}
\usepackage{mathrsfs}
\usepackage{array}
\usepackage{gensymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \cvprfinalcopy % *** Uncomment this line for the final submission
\newcommand{\figref}[1]{Fig\onedot~\ref{#1}}
\newcommand{\equref}[1]{Eq\onedot~\eqref{#1}}
\newcommand{\secref}[1]{Sec\onedot~\ref{#1}}
\newcommand{\tabref}[1]{Tab\onedot~\ref{#1}}
\newcommand{\thmref}[1]{Theorem~\ref{#1}}
\newcommand{\prgref}[1]{Program~\ref{#1}}
\newcommand{\algref}[1]{Alg\onedot~\ref{#1}}
\newcommand{\clmref}[1]{Claim~\ref{#1}}
\newcommand{\lemref}[1]{Lemma~\ref{#1}}
\newcommand{\ptyref}[1]{Property\onedot~\ref{#1}}

\newcommand{\ve}[1]{{\mathbf #1}} % for displaying a vector or matrix
\newcommand{\hua}[1]{{\mathcal #1}}
\newcommand{\scr}[1]{{\mathcal #1}}
\newcommand{\by}[2]{\ensuremath{#1 \! \times \! #2}}

%\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\onedot{\ifx\@let@token.\else.\null\fi\xspace}
\def\eg{\emph{e.g.}} 
\def\Eg{\emph{E.g}\onedot}
\def\any{\forall}
\def\ie{\emph{i.e.}} 
\def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{cf}\onedot} 
\def\Cf{\emph{Cf}\onedot}
\def\etc{\emph{etc}\onedot} 
\def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} 
\def\dof{d.o.f\onedot}
\def\etal{\emph{et al.}}

\def\cvprPaperID{936} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{DeepLocSeg: Online Camera Pose Estimation and Scene Parsing with Deep Learning and a 3D Semantic Map}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
Visual-based outdoor navigation requires accurately localizing the camera and preferably per-pixel semantic understanding, which can be widely applied for autonomous driving, or augment reality \etc.
%However, system solely relying on visual signal is non-robust due to visual confusion across multiple scenes. 
%Thus, coarse signals from motion sensors, \eg GPS and IMU, are usually considered as a localization prior~\cite{}.
In this paper, we propose a deep learning based method for localizing the camera and parsing the recorded video simultaneously in a single framework, which fuses signals from camera and motion sensors like GPS and IMU.
Specifically in our setting, we have a 3D semantic world, and a video is recording online inside, meanwhile a non-accurate camera pose signal is observed. 
In our system, at each timestamp, based on the obtained pose signal, we render a label map out of the 3D world, and feed it to a pose CNN jointly with the current frame of image, yielding a corrected camera pose. 
Then, a multi-layer recurrent neural network (RNN) is performed afterwards, which captures high-order temporal information that further improve the pose accuracy. 
Finally, based on the corrected pose from RNN, a new label map is rendered, and is fed into a segment CNN combining with the image, yielding a parsing of the scene.
In order to perform the experiments,  we build a dataset with a semantic labeled real world 3D map, jointly with many recorded videos with ground truth poses from high accurate motion sensors. 
We show that practically, pose estimation solely relying on images like PoseNet~\cite{Kendall_2015_ICCV} may fail due to street view confusion, and it is important to fuse multi sensors. In addition, semantic parsing and pose estimation are mutually beneficial in learning more robust networks. Finally, various ablation studies are performed, which demonstrate the effectiveness of the proposed system.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:introduction}
% the problem we are solving, distinguish with previous problems
In the applications like street navigation~\cite{ohno2003outdoor}, visual-based 6-DOF camera pose estimation~\cite{campbell2017globally,moreno2008pose,Kendall_2015_ICCV,coskun2017long} providing high accuracy attracts much attention in computer vision. 
Additionally, to acquire better scene understanding for semantics applications such as augment reality~\cite{DBLP:journals/corr/abs-1708-05006}, parsing each frame of a video is also important. 

\begin{figure*}[t]
\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
   \caption{Framework of our proposed system. The black arrows show the testing process and red arrows show the back-propagation in training.}
\label{fig:framework}
\end{figure*}

% existing methods only consider one of the tasks with soly visual signal. 
Currently, most existing vision algorithms are trying to solve both of the tasks solely depend on visual signals. 
For instances, geometric based methods are relying on visual feature matching, \eg system of Perspective-n-Points (PnP)~\cite{haralick1994review,kneip2014upnp,campbell2017globally} when a 3D map and an image is provided, or systems of SLAM~\cite{engel2014lsd,mur2015orb,NewcombeLD11} when it is a video. Such systems are relying on local appearance, which could fail when confronted of low-texture environments.
 
Most recently, deep learning based methods, \eg either for images~\cite{Kendall_2015_ICCV} or videos~\cite{DBLP:journals/corr/ClarkWMTW17}, are proposed for real-time localization, which show much better trade-off between accuracy and speed.
Nevertheless, those monocular methods are good for environments with rich distinguishable features, \eg Cambridge landmarks~\cite{Kendall_2015_ICCV}, while could fail for common street views with very similar appearances. For example, when driving forward with same trees beside, can hardly localize the images without external signals.

For scene parsing, approaches~\cite{ZhaoSQWJ16,ChenPSA17} based on deep fully convolutional network (FCN) with ResNet~\cite{HeZRS15} are the best algorithms for single image according to Dataset like CityScape~\cite{Cordts2016Cityscapes}. When input is videos, researchers~\cite{kundu2016feature,zhu2016deep} may incorporate optical flow between consecutive frames, which accelerates the parsing, and builds the consistency along the temporal dimension. Furthermore, for background, one may factorize the flow to 3D map and pose through structure-from-motion (SFM)~\cite{wu2011visualsfm}, so that jointly parsing and reconstruction~\cite{kundu2014joint} can performed. However, these methods are still not efficient enough and facing challenges of scene variations cross different weathers and cities for real applications.

In our scenario, targeting at a more practical setting, we consider the video is online recorded, and a 3D semantic map is pre-built by us. For handling the localization confusion of street views, we propose to fuse signals from motion sensors like global positioning system (GPS) and inertial measurement unit (IMU), which is typically available for current navigation system. Those signals can be noisy but is crucial as a pose priori for our deep learning system. 
For video segmentation, we online render images from the 3D map, which serves as a priori for further segmentation, and helps the consistency along the temporal dimension. 

% 
In fact, our problem setting is on par with the widely applied mobile navigation system, whereas the 2D labeled map is raise up to a 3D semantic map, and the problem of 2D pose estimation is changed to 3D camera pose. Promisingly, lots of city scale 3D map has already collected from companies such as Google Earth~\cite{sheppard2009ethics} and Altizure\footnote{https://www.altizure.com/}, and also semantic labeled ones is also built such as Toronto city~\cite{wang2016torontocity}. In our case, we constructed our own data with high quality 3D semantic map, by adopting a high accurate LIDAR device called Riegl\footnote{http://www.rieglusa.com/index.html}.

Last but not the least, within our deep learning framework, the camera pose and semantics are mutually beneficial, where pose helps build the correlation between the 3D semantic map and 2D semantic segments. Reversely, semantics could help align camera poses, yielding better results for both tasks than doing them individually. In our experiments, using a single core of Titan Z GPU, our system estimates the pose in 10ms with accuracy under 1 degree, and segments the image $512 \times 608$ in less than 100ms with pixel accuracy around 96$\%$, which demonstrates its efficiency and effectiveness.

% redefine our problem
In summary, the contributions of this paper are in three folds:
\begin{itemize}
    \item We propose a deep learning based system for fusing multiple information, \ie camera, GPS and IMU, and 3D maps, which helps to improve the robustness and accuracy for camera localization and scene parsing.
    \item In our system, camera pose and scene semantics are handled jointly in both training and testing, where two kinds of information are mutually beneficial.
    \item To evaluate based on the setting of problem, we construct a dataset from real scenes. It includes dense 3D semantically labelled point cloud, ground truth camera poses and pixel-level parsing of every frame, which we will release in order to benefit the related research in computer vision.
\end{itemize}

The structure of this paper is organized as follows. We first give a overview of our system in \secref{sub:framework}. In \secref{sec:data_collection}, we first describe how our data is different from the existing outdoor datasets. In addition, we briefly introduce the method how the data is collect and labelled. Then, \secref{sec:localize_and_parsing} presents the framework and detail of our system. The full performance is evaluated quantitatively for both pose estimation and parsing in \secref{sec:experiments}, and \secref{sec:conclusion} concludes the paper and point out future directions. Finally, We will release all our code, models and dataset with the publication of this paper.


\subsection{Framework}
\label{sub:framework}
The framework of our system is illustrated in \figref{fig:framework}. At above, a pre-built 3D semantic map is prepared. During testing, an online stream of images and corresponding camera poses are fed in to the system. Firstly, for each frame, a semantic label map is rendered out based on the obtained coarse camera pose. Then it is fed to a pose network jointly with the respective image.  The network calculates their relative rotation and translation, and yields a corrected camera pose. To build the temporal correlations, the corrected poses are fed into a RNN which further improve the pose accuracy in the stream. 
Last, given the rectified camera pose, a new rendered label map is generated. We feed it together with the image to a segmentation network, which helps to segment a spatially more accurate and temporally more consistent result for the stream of video. 
In this system, since our data contains ground truth for both pose and segments, it can be trained with strong supervision at each end of outputs.

\section{Related work}
\label{sec:related_work}
Estimating camera pose and parsing images with a video or a single image have long been center problems for computer vision and robotics. 
Here we summarize the correlated works in several aspects without enumerating all works in relative field due to space limitation. 
In our problem, we are interested in visual-based street-view cases, thus localization and general parsing in the wild is out of our scope.

% talk each perspective with image and video
\textbf{Camera pose estimation.} Traditionally, localizing an image given a set of 3D points is formulated as a Perspective-$n$-Point (P$n$P) problem~\cite{haralick1994review,kneip2014upnp} by matching feature points in 2D and features in 3D through cardinality maximization. Usually in a large environment, a pose prior is required in order to obtain good estimation~\cite{david2004softposit,moreno2008pose}. Campbell \etal~\cite{campbell2017globally} propose a global-optimal solver which leverage the prior. At the case that geo-tagged images are available, Sattler \etal~\cite{sattler2017large} propose to use image-retrieval to avoid matching large-scale point cloud.
When given a video, relative pose could be further modeled with methods like SLAM~\cite{engel2014lsd} etc, which increases the localization accuracy and speed. 

Although these methods are effective in many cases with distinguished feature points, it is still not practical for city-scale environment with billions of points, and also may fail in low texture, thin structure and occlusions. 
Thus, recently, deep learned features with CNN is proposed to combine low-level and high-level feature for localization. PoseNet~\cite{Kendall_2015_ICCV,kendall2017geometric} takes an low-resolution image as input, which can estimate pose in 10ms \wrt a feature rich environment composed of distinguished landmarks. LSTM-PoseNet~\cite{hazirbasimage} further captures a global context after CNN features.
Given an video, later works incorporate Bi-Directional LSTM~\cite{DBLP:journals/corr/ClarkWMTW17} or Kalman filter LSTM~\cite{coskun2017long} to obtain better results with temporal information. However, in street-view scenario, considering a road with trees aside, in most cases, no significant landmark appears, which could fail the visual models. Thus, although noisy, signals from GPS/IMU are a must assistant for localization in these cases~\cite{vishal2015accurate}, whereas the problem changes to estimating the relative pose between the camera view of a noisy pose and the real pose. To handle this, recently, Researchers ~\cite{laskar2017camera,ummenhofer2016demon} proposes to concatenate two real images as a network input, while in our case, we concatenate the real image with online rendered label map from the noisy pose, which provides superior results in our experiments.

\textbf{Scene parsing.} For parsing a single image, most state-of-the-arts (SOTA) algorithms over street-view datasets like CityScapes~\cite{Cordts2016Cityscapes} are designed based on a FCN~\cite{WuSH16e} and a multi-scale context module with either dilated convolution~\cite{ChenPSA17}, pooling~\cite{ZhaoSQWJ16}, CRF~\cite{} or spatial RNN~\cite{byeon2015scene}. However, they are dependent on a ResNet~\cite{HeZRS15} with hundreds of layers, which is comparably too heavy for real applications like augment reality. Some researchers apply small models~\cite{PaszkeCKC16} or model compression~\cite{ZhaoQSSJ17} for acceleration by sacrificing the accuracy. 
When the input is a video, spatial-temporal graph is built, Kundu \etal~\cite{kundu2016feature} use 3D dense CRF to get temporal consistent results. Recently, optical flow~\cite{dosovitskiy2015flownet} between consecutive frames is computed to transfer label or features~\cite{gadde2017semantic,zhu2016deep} from previous frame to current.  Significant progress has been made, yet the consistency is built through flow, rather than 3D, and camera pose, which is a more compact information to use. In our case, we use the rendered label map from 3D as the prior to the segment network, which leverage the difficulty of scene parsing solely from image cue. For practicality, we also adopt the light weighted network from DeMoN~\cite{ummenhofer2016demon} to keep the system consistent.


\textbf{Joint 2D-3D for video parsing.} Another set of works that could be related with ours is joint reconstruction, pose estimation and parsing~\cite{kundu2014joint,hane2013joint} through 2D-3D consistency supervised training. 
 Traditionally, those methods are reliance on structure-from-motion (SFM)~\cite{hane2013joint} from feature or photometric matching. Specifically, they reconstruct a 3D map and performing semantic parsing over 2D and 3D jointly, yielding consistent segmentation between multiple frames. 
 Most recently, CNN-SLAM~\cite{tateno2017cnn} replace the 3D reconstruction using a depth network from a single image, and a segment network for image parsing.
 However, all there approaches is processed offline and only for static background, which does not satisfy our online setting. Moreover, the quality of reconstructed 3D is not comparable with the one directly collected from real world.
% traditional key points, slam etc
% pnp
% marc's paper
% hongdong's paper
% silvio's paper
% posenet related
% jifeng's paper
% iccv kalman 
% domain transfer etc


\section{Constructing the data}
\label{sec:data_collection}
\begin{table*}[t]
\center
\begin{tabular}{lccccc}
\toprule[0.2 em]
% \thickhline
Dataset & Real data & Camera pose & 3D map & Video per-frame labeling  & Temporal variations  \\ 
\hline 
\multicolumn{1}{l|}{CamVid~\cite{brostow2009semantic}}     &\checkmark                       & -              & -              &  -  & -  \\
\multicolumn{1}{l|}{KITTI~\cite{geiger2012we}}      &\checkmark  & \checkmark     & sparse points  & -   & -  \\
\multicolumn{1}{l|}{CityScapes~\cite{Cordts2016Cityscapes}} &\checkmark  & -              &  -             & selected frames & - \\
\multicolumn{1}{l|}{Toronto~\cite{wang2016torontocity}}    &\checkmark  & \checkmark     & sparse points  & selected pixels & - \\
\hline
\multicolumn{1}{l|}{Synthia~\cite{RosCVPR16}}    & -          & \checkmark     & -       &\checkmark & -    \\ 
\multicolumn{1}{l|}{Play for benchmark~\cite{richter2017playing}} &-   & \checkmark     & -     &\checkmark & - \\
\hline 
\multicolumn{1}{l|}{Ours}              & \checkmark &\checkmark    &dense point cloud  & \checkmark     &  \checkmark \\
\toprule[0.2 em]
\end{tabular}
\caption{Compare our data with the other related outdoor street-view datasets for our task. 'Real data' mean whether the data is collected from realistic world. 
'3D map' means whether it contains 3D map of the whole dataset. 'Video per-frame labeling' means whether it has per-frame per-pixel semantic label. 
'Temporal variations' mean whether the recorded video can roughly cover the whole scene, but have multiple.}
\label{tbl:data}
\vspace{-0.3\baselineskip}
\end{table*}

\paragraph{Motivation.}
As described in the \secref{sub:framework}, our system is performed with available motion sensors and a semantic 3D map. 
However, similar outdoor dataset produced by prior arts such as KITTI and CityScapes, does not containing such information. As summarized in \tabref{tbl:data}, we list several key properties to perform our experiments. 
As can be seen, existing ones do not satisfy our requirements, especially for pose estimation, for training, we want the recorded video to roughly cover the 3D environment, so that when new images come in, appearance similar views had been seen during training. This means repeated recording at similar spatial locations are required in the data, which we call temporal variations in \tabref{tbl:data}.
However, most existing datasets are a temporal snapshot at some locations in the environment, which requires us to collect a new dataset ourselves.

\paragraph{Data collection.}
We use a LIDAR device called $Riegl$ to collect point clouds of the static 3D map with high granularity. As shown in \figref{fig:data}(a). The captured point cloud density is much higher than the Velodyne\footnote{http://www.velodynelidar.com/} used by KITTI~\cite{geiger2012we}. 
% Amazingly, it is even able to capture the hight changes of curb on road. 
However, different from the sparse LIDAR, the laser of $Reigl$ scanning is horizontally raising from bottom the top, while moving objects will not be captured instantly with a single snapshot, yielding fake points for moving objects. 
In order to remove those fake object points, thanks to the fact that the 3D world is recorded multiple rounds, and each round has a full coverage of the 3D map, we can use point clouds consistency to handle the issue.
Specifically, we calibrate the 3D point clouds from different rounds, and put all the points together. From the merged point cloud, the points with high temporal consistency from other rounds are kept, while those with low consistency are removed. Formally, the condition to kept a point $\ve{x}$ in round $j$ is,
\begin{align}
\sum_{i=0}^{r}{\mathbbm{1}(\exists~\|\ve{x}_i - \ve{x}_j\| < \epsilon_d )} / r \geq \delta
\end{align}
where $\delta = 0.7$ and $\epsilon_d = 0.025$ in our experiments, and $\mathbb{1}()$ is a indicator function. Notice due to the symmetric property of $\ve{x}_i$ and $\ve{x}_j$, we obtain the static points for all rounds by looping over the points of a single round. We keep the merged point cloud as a static background $\hua{M}$ for further labelling.

For video, we use two cameras facing towards direction of street with a resolution of \by{2048, 2432}, and both cameras are well calibrated \wrt the LIDAR. Based on provided camera parameters from the hardware, all the images from videos are undistorted to match the 3D points.
% talk about registration and removing moving objects inside data

% Undistortion of the image
\paragraph{Label 3D map and videos.}
% talk about labelling in 3D
In order to have semantic labelling of each frame in video, we handle static background, static objects and moving objects separately. 
Firstly, for static background, we directly do labelling on the 3D point cloud $\hua{M}$ which is then projected to images, yielding labelled background for all the frames.
Specifically, we over-segment the point cloud data into point clusters based on spatial distance and normal directions, and then we ask labellers to label each cluster of points manually. 
For static objects in each round of video, we prune out the points of static background, and label the remaining points of objects. Due to the fact that when object is static, \eg parked cars, the shape and location of captured points will be highly recognizable, whereas when the object is moving on the road, the points will be fuzzy, we ask labeller to label those can be well recognized as static objects. Last, after projecting the 3D to 2D, only moving objects are remaining to label. Here, we adopt an active labelling strategy, by first training an object segmentation model using a SOTA algorithm~\cite{WuSH16e}, and ask labellers to correct and label the masks of moving objects.

Finally, as shown in \figref{fig:data}(c), the projected label from 3D points is not good enough, due to missing points either too far away or reflection. We handle such issue by projecting small squares rather than points as discussed in \secref{sub:render} (\figref{fig:data}(d)), and then ask labeller to fix missing regions, yielding the final label map (\figref{fig:data}(e)).
With such a strategy, labelling efficiency of video can be vastly increased, avoiding labelling edge rich regions like trees and poles on the street, especially when occlusion is happened.
We provided the labelled video in our supplementary materials for readers who are interested. 

\begin{figure}[t]
\begin{center}
\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
\end{center}
   \caption{Snapshot of our collected data. (a) 3D semantic map. (b) Images. (c) Rendered label map with 3D points (d) Rendered label map with enlarged square. (e) Merge label map with object and sky.}
\label{fig:data}
\end{figure}

\section{Localizing camera and scene parsing.}
\label{sec:localize_and_parsing}
As shown in \secref{sub:framework}, our full system is based on a semantic 3D map and deep CNN. In the following, we will first describe how a semantic label map is rendered from the 3D, then talk about the details of our network architecture and the loss functionals to train the system.

\subsection{Render a label map from a camera pose.} 
\label{sub:render}
Formally, given a 6-DOF camera pose $\ve{p} = [\ve{q}, \ve{t}] \in SE(3)$, where $\ve{q} \in SO(3)$ is the quaternion representation of rotation and $\ve{t} \in \mathbbm{R}^3$ is translation, a label map can be rendered from the semantic 3D map, where z-buffer is applied to find the closest point at each pixel.

In our setting, the 3D map is a point cloud based environment which includes millions of points. Although the density of the point cloud is very high (one point per 2.5 centimeters within road regions), when the 3D points are far away from the camera, the projected points could be very sparse, \eg regions of buildings etc.
Thus for each point in the environment, we enlarge the 3D point to a square piece where the square size is dependent on its semantic class. Formally, for a 3D point $\ve{x}$ belonging a class $c$, the square size $s_c$ is set to be,
\begin{align}
\label{eq:square_size}
s_c \propto \frac{1}{|\hua{P}_c|}\sum_{\ve{x}\in \hua{P}_c} \min_{\ve{t}\in\hua{T}} d(\ve{x}, \ve{t})
\end{align}
where $\hua{P}_c$ is the set of 3D points belong to class $c$, and $\hua{T}$ is the set of camera locations. Then, given the proportion, we rescale the square size within a range between a minimum size and a maximum size, which is set to be $[0.025, 0.05]$.

With such a strategy, as shown by \figref{fig:data}, regions with invalid value in-between projected points are in-painted, meanwhile the the boundaries between different segments are also kept. By feeding in such a rendered map, it facilitates the CNN to better discover scene layouts both for pose estimation and scene parsing.
One may wonder why we render semantic label map rather than synthesizing an RGB image for later inputs. 
This is because intuitively label maps is invariant to color changes, which yields better boundaries reflecting the scene layout. 

\subsection{Camera Localization with motion prior}

\begin{figure}[t]
\begin{center}
\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
\end{center}
   \caption{The GRU RNN network architecture for modeling a sequence of camera poses.}
\label{fig:rnn}
\end{figure}

\paragraph{Pose rectify with road prior.} Firstly, 

\paragraph{CNN-GRU pose network architecture.} 
As shown in \figref{fig:framework}, the CNN of our pose network takes as inputs an image $\ve{I}$ and the rendered label map $\ve{L}^c$ from corresponding coarse camera pose $\ve{p}_i^c$. It outputs a 7 dimension vector $\hat{\ve{p}}_i$ representing the relative pose between the image and rendered label map, and the estimated pose respect the world is $\ve{p}_i = \ve{p}_i^c + \hat{\ve{p}_i}$.
For designing the network, in order to have large kernel to obtain bigger context while keeping the amount of parameters and runtime manageable, we follow the design of Ummenhofer \etal~\cite{ummenhofer2016demon}. The convolutional kernel of this network consists a pair of 1D filters in $y$ and $x$-direction, and the encoder gradually reduces the spatial resolution with stride of 2 while increasing the number of channels. We list the details of the network in our implementation details at \secref{sec:experiments}.

Additionally, after the output pose $\ve{p}_i$ from our pose network, as illustrated in \figref{fig:rnn}, a forward GRU-RNN is performed to model the temporal motion sequence of the video, since the movement of a vehicle is commonly smooth.
Traditionally in navigation applications of estimating 2D poses,  Kalman filter~\cite{kalman1960new} is applied by calculating either constant velocity or acceleration. 
In our case, transition of camera poses is learned from the training sequences, where the current motion is predicted by the RNN from a sequence of previous predicted poses, which yields further improvement over the estimated poses from the pose CNN.


\paragraph{Pose loss.} 
Following the PoseNet~\cite{}, we use the geometric matching loss for training, which avoids the balancing factor between rotation and translation. 
Formally, given a set of point cloud in 3D $~\hua{P}=\{\ve{x}\}$, and the loss for each image is written as,
\begin{align}
L(\ve{p}, \ve{p}^*) = \sum_{\ve{x} \in \hua{P}}\omega_{l_\ve{x}}|\pi(\ve{x}, \ve{p}) - \pi(\ve{x}, \ve{p}^*)|_2
\label{eq:proj_loss}
\end{align}
where $\ve{p}$ and $\ve{p}^*$ are the estimated pose and ground truth pose respectively. $\pi()$ is a projective function that maps a 3D point $\ve{x}$ to 2D image coordinates. $l_\ve{x}$ is the semantic label of $\ve{x}$ and $\omega_{l_\ve{x}}$ is a weight factor depended on the semantics. Here, we set stronger weights for point cloud belong to certain classes like traffic light, and we found it gives better performance for estimated poses.
In ~\cite{kendall2017geometric}, usually the 3D points that is visible by the camera are used which helps the stableness of learning. In our case, online searching the visible ones in millions of points are infeasible. 
Thus, we pre-render a depth map for each training image with resolution of $256 \times 304$, using the ground truth pose, and select those points projected on the depth map for training.

% Intuitively, we find the amount of points in each class is dramatically unbalanced, and most points are those on roads and trees. Nevertheless, the appearance variation of road and trees are not very sensitive to pose changes in the street-view scenario. In order to encourage the network to discover more str
% Thus,  other structures images like electricity pole or road light have rich structures like edges and textures, which potentially should be valued more for matching.
% In~\cite{ummenhofer2016demon}, the model try to predict a flow confidence map revealing the texture rich regions which helps pose estimation. In our work, we adopt the labelled semantic labels, and reweight each 3D point in the training loss, which drives the network to focus more on 
% Thus, the learning loss changed to 
% \begin{align}
% L(\ve{p}, \ve{p}^*) = \sum_{\ve{x} \in \hua{P}}\omega_{l_\ve{x}}|\pi(\ve{x}, \ve{p}) - \pi(\ve{x}, \ve{p}^*)|_2
% \label{eq:proj_loss}
% \end{align}
% where $\omega_{l_\ve{x}}$ is the weight of class $l_\ve{x}$. We set the weight for each class depends on the class edgeness which is the percentage of pixel along the edge 
\subsection{Video parsing with pose guidance}
Having rectified pose at hand, one may direct render the semantic 3D world to current view of a camera, yielding a semantic parsing of the current image. However, the estimated pose is not perfect, thus alignment of very think regions like light pole can totally fail. Another fact is that many LIDAR points are missing due to reflection, \eg regions of building mirrors, and points can be sparse at long distance. Finally, non-linear distortion of images and 3D map could happen due to system error, yielding scene miss alignment. Thus, we propose to a parameter light segment network to handle these issues. 
In our experiments, we show with pose rendered label map as an additional input, the segment results are temporally more consistent and yields better accuracy.

\begin{figure*}[]
\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
   \caption{Architecture of the segment network.}
\label{fig:segnet}
\end{figure*}
\paragraph{Segment network architecture.} As illustrated in \figref{fig:segnet}, the segment network contains an encoder-decoder network and a refinement network, and both have similar architecture with the corresponding ones used in DeMoN~\cite{ummenhofer2016demon}. 
The difference for encoder-decoder network is that for network inputs, rather than directly concatenate the label map with input image, to balance the two, we first transform the label map to a score map through one-hot operation, and embed the score of each pixel to a 32 dimensional feature vector. Then, we concatenate this feature vector with output features from those of the first layer, and keeps the rest as the same with that in~\cite{ummenhofer2016demon}. For refinement network, we use the same strategy to handle the two inputs. Finally, the segment network produce a score map, yielding the semantic label of each pixel.

We train the segment network firstly with only RGB images, then fine-tune the network by adding the input of rendered label maps. This is because our network is trained from scratch, thus needs large amounts of data to learn effective features from the image. However, in our experiments, the rendered label map from the estimated pose has on average 82$\%$ pixel accuracy. This could easily drive the network fitting the input label map at first, while slow down the process towards learning features from images. Finally, for segmentation loss, we use the standard softmax loss, and add intermediate supervision right after the output from encoder and decoder as indicated in \figref{fig:segnet}.


\section{Experiments}
\label{sec:experiments}
We perform all our experiments using our collected dataset, and evaluate multiple system settings for pose and segment to validate each component. 
For obtaining GPS and IMU signal, due to at each time step, one sampled noisy signal can obtained. This is limited for training the system. Thus, follow~\cite{vishal2015accurate}, we simulate GPS and IMU error by adding random noise $\epsilon$ with uniform distribution. Specifically, translation and rotation noise are set as $\epsilon_t \sim U(0, 7.5)$ and rotation $\epsilon_r \sim U(0^{\circ}, 15^{\circ})$ respectively. We refer to realistic data~\cite{lee2015gps} for setting range of simulation noise.

In this paper, our data vehicle collected 6 videos at different daytimes surrounding a technology park. The 3D map generated in our experiment has a road length around 3500$m$, and the distance between consecutive frames is around 5$m$ to 10$m$. We use 4 of the videos for training and 2 for testing, yielding 2242 training images and 756 testing images. The semantic classes included in the dataset includes $\{$sky, car-lane, ped-lane, bike-lane, curb, traffic-cone, traffic-stack, traffic-fence, light-pole, traffic-light, tele-pole, traffic-sign, billboard, building, security-stand, plants, object $\}$. In the future, we are targeting at constructing even larger datasets with more semantics.

\paragraph{Implementation details.} To quickly render from the 3D map, we adopt OpenGL to efficiently render a label map while handling the z-buffer operation. A 512 $\times$ 608 image can be generated in 70ms with a single Titan Z GPU, which is also the input size for both pose network and segment network. For segment output, we keep the size the same as input. Both of the network is learned with 'Nadam' optimizer~\cite{dozat2016incorporating} with a learning rate $10^{-3}$. For training pose RNN, we sample a video sequence with 100 frames from the training data. We sequentially train the three models due to GPU memory limitation.
For pose CNN and segment CNN, we stops at 100 epochs, and for pose RNN, we stops at 200 epochs. For data augmentation, we use the imgaug\footnote{https://github.com/aleju/imgaug} library to variate lighting, blurring and flipping. We select a subset from training images for validation trained model from each epoch, and choose the model performing best for evaluation.

For testing, to get a confidence range for the prediction for verifying the effectiveness of each model, we report the standard variation of the results from a 10 time simulation. Finally, we implement all the networks by adopting the MXNet~\cite{ChenLLLWWXXZZ15} platform. 


\paragraph{Pose Evaluation.}
In \tabref{tbl:pose}, we show the pose estimated results. We first direct following the work of PoseNet~\cite{Kendall_2015_ICCV,kendall2017geometric}, and use their published code to train a model. Due to scene appearance similarity of the street-view (\figref{fig:data}), it does not give reasonable results, \ie better than the noisy GPS and IMU signal, in our experiments. 
By using only CNN and coarse pose prior, the model can start to learn reasonable relative poses (3rd row), which significantly reduces the pose error. When inserting semantic cues such as road priori and semantic weights in \equref{eq:proj_loss}, the pose results is significantly improved, especially at rotation (4th row). 
Finally, RNN gives strong cues about the moving speed and acceleration of the camera, yields the best results for both translation and rotation. 

\begin{table*}
\center
\small
\begin{tabular}{lccc}
\toprule[0.1 em]
% \thickhline
Method & Trans (m) & Rot ($\circ$) & Pix. Acc($\%$)\\ 
\hline 
PoseNet~\cite{kendall2017geometric} & -  & -  & -  \\
Noisy pose & 3.45 $\pm$ 0.076 & 7.87 $\pm$ 0.10 & 54.01 $\pm$ 0.5 \\
Pose RNN w/o CNN & 1.282 $\pm$ 0.061  & 1.731 $\pm$ 0.06 &  68.1 $\pm$ 0.32 \\
Pose CNN w/o semantic & 1.355 $\pm$ 0.052  & 0.982 $\pm$ 0.023 & 70.99 $\pm$ 0.18 \\
Pose CNN w semantic & 1.331 $\pm$ 0.057  & 0.727 $\pm$ 0.018 & 71.73 $\pm$ 0.18  \\
Pose CNN-RNN  & \textbf{1.005} $\pm$ 0.044  & \textbf{0.719} $\pm$ 0.035  & \textbf{73.01} $\pm$ 0.16  \\
\toprule[0.1 em]
\end{tabular}
\caption{Compare the accuracy of different settings for pose estimation. 
Noisy pose indicates the input signal from GPS, IMU. 
The number after $\pm$ indicates the std from 10 simulations. We can see the improvement is statistically significant.}
\label{tbl:pose}
\vspace{-0.3\baselineskip}
\end{table*}

\paragraph{Segment Evaluation.}
In \tabref{tbl:segment}, we show the scene parsing results. We first use one of the state-of-the-art parsing network on the CityScapes \ie ResNet38~\cite{WuSH16e}. It has pre-trained parameters from both ImageNet and CityScapes, running with a 5s per-frame with our resolution. Although the results could be good, it is too slow for real-applications. Our network can run in 0.1s, while yields comparable results (4th row). At 2nd row, we use the ground truth pose to obtain an upper-bound for the segmentation performance. In this case, the projected label map aligns perfect with the image, thus yields significantly better results than the rest. At 3rd row, we train the segment network without pose prior.  Notice that for a fair comparison, after convergence at 100 epoch, we continue train the network another 100 epochs in order to avoid the influence from longer training. 
At 4th and 5th row, we show the results train with rendered label map from pose CNN and pose CNN-RNN respectively, where better pose can help the segment accuracy get closer to the oracle case, which demonstrate the effectiveness of our strategy. Due to space limit, we leave per-class results to the supplementary.

% \begin{table*}
% \center
% \begin{tabular}{lcccccccccccccccccc}
% \toprule[0.1 em]
% %\thickhline
% Method & mean IOU & sky & car-lane & ped-lane & bike-lane & curb & $t$-cone & $t$-stack & $t$-fence & light-pole & traffic-light & tele-pole & $t$-sign & ad-board & building & security-stand & plants & object \\ 
% \hline 
% ResNet38~\cite{} & & &  & & & & & & & & & & & & & & &  \\
% Segment CNN w oracle pose & & & & & & & & & & & & & & & & & &  \\
% Segment CNN w/o pose & & & & & & & & & & & & & & & & & &  \\
% Segment CNN w pose CNN & & & & & & & & & & & & & & & & & & \\
% Segment CNN w pose CNN-RNN & & & & & & & & & & & & & & & & & & \\
% \toprule[0.1 em]
% \end{tabular}
% \caption{Compare the accuracy of different segment networks setting. $t$ is short for 'traffic' in the table. $\pm$ indicates the confidence region by 10 time GPS simulation.}
% \label{tbl:segment}
% \vspace{-0.3\baselineskip}
% \end{table*}

\begin{table}[t]
\center
\small
\begin{tabular}{lccc}
\toprule[0.1 em]
%\thickhline
Method &  mIOU ($\%$) & mAcc ($\%$) & Pix. Acc ($\%$)\\
\hline 
ResNet38~\cite{WuSH16e} & & &  \\
SegCNN w/o Pose & & & \\
SegCNN w pose GT & & & \\
SegCNN w Pose CNN & & & \\
SegCNN Final & & & \\
\toprule[0.1 em]
\end{tabular}
\caption{Compare the accuracy of different segment networks setting. $\pm$ indicates the confidence region by 10 time GPS simulation.}
\label{tbl:segment}
\vspace{-0.3\baselineskip}
\end{table}

\paragraph{Qualitative results.} In \figref{fig:results}, we select a subsequence composed of 5 frames to show the results. More results are provided in the supplementary material. At 2nd and 3rd column, we show how the pose network aligns the image with pose from view of camera. At 4th and 5th column, we compare the segment results taking only images as input and those also consider rendered label map. As can be seen at the circled regions, the segment results with pose rendered label maps not only give better accuracy, but also have better temporal consistency.
\begin{figure*}[t]
\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
   \caption{Results from each intermediate stage out of the system.}
\label{fig:results}
\end{figure*}


\section{Conclusion}
\label{sec:conclusion}
In this paper, we present a unified strategy for performing video localization and segmentation with deep CNN and a 3D semantic map. We show that 

\paragraph{Future work.} First, we can follow the LSD-SLAM~\cite{engel2014lsd} to further re-localize the camera through perpixel semantic matching, rather than photometric matching, which could potentially further improve the accuracy. In addition, we would like to simplify the 3D point cloud map by modeling the surfaces, based on which a better and faster rendered label map can generated.
Finally, additional to a relative small data collected in this paper, significantly larger and harder data will be collected. and a much more detailed and concrete description \wrt scaling up the collection will be presented in the near future. 

% 
{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}

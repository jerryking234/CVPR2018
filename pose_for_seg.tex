\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{url}
\usepackage[table]{xcolor}
\usepackage{bbm}
\usepackage{booktabs}
\usepackage[T1]{fontenc}
\usepackage{fix-cm}
\usepackage{array}
\usepackage{epsfig}
%\usepackage{mathabx}
\usepackage{dsfont}
\usepackage{multirow}

\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{color}
\usepackage{epstopdf}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{calc}
\usepackage{multirow}
\usepackage{xspace}
\usepackage{booktabs}
\usepackage{mathrsfs}
\usepackage{array}
\usepackage{gensymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \cvprfinalcopy % *** Uncomment this line for the final submission
\newcommand{\figref}[1]{Fig\onedot~\ref{#1}}
\newcommand{\equref}[1]{Eq\onedot~\eqref{#1}}
\newcommand{\secref}[1]{Sec\onedot~\ref{#1}}
\newcommand{\tabref}[1]{Tab\onedot~\ref{#1}}
\newcommand{\thmref}[1]{Theorem~\ref{#1}}
\newcommand{\prgref}[1]{Program~\ref{#1}}
\newcommand{\algref}[1]{Alg\onedot~\ref{#1}}
\newcommand{\clmref}[1]{Claim~\ref{#1}}
\newcommand{\lemref}[1]{Lemma~\ref{#1}}
\newcommand{\ptyref}[1]{Property\onedot~\ref{#1}}

\newcommand{\ve}[1]{{\mathbf #1}} % for displaying a vector or matrix
\newcommand{\hua}[1]{{\mathcal #1}}
\newcommand{\scr}[1]{{\mathcal #1}}
\newcommand{\by}[2]{\ensuremath{#1 \! \times \! #2}}

%\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\onedot{\ifx\@let@token.\else.\null\fi\xspace}
\def\eg{\emph{e.g.}} 
\def\Eg{\emph{E.g}\onedot}
\def\any{\forall}
\def\ie{\emph{i.e.}} 
\def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{cf}\onedot} 
\def\Cf{\emph{Cf}\onedot}
\def\etc{\emph{etc}\onedot} 
\def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} 
\def\dof{d.o.f\onedot}
\def\etal{\emph{et al.}}

\def\cvprPaperID{936} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{DeepLocSeg: 6-DOF Camera Pose Estimation and Scene Parsing with 3D Semantic Map and Deep CNN}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
Visual-based outdoor navigation requires accurately localizing the camera and preferably per-pixel semantic understanding. It can be widely applied for autonomous driving, or augment reality navigation \etc.
%However, system solely relying on visual signal is non-robust due to visual confusion across multiple scenes. 
%Thus, coarse signals from motion sensors, \eg GPS and IMU, are usually considered as a localization prior~\cite{}.
In this paper, we propose a deep learning based method for localizing the camera and parsing the recorded video simultaneously in a single framework, which fuses signals from camera and motion sensors like GPU and IOU.
Specifically, assuming we have a 3D semantic world, and the testing video is recording inside. Rough pose signal is also obtained online. In our system, for each frame in a moving camera, based on the obtained coarse pose signal, we render a label map out from our 3D world online, and feed it to a pose CNN jointly with the current frame of image, yielding a corrected pose. 
Then, a multi-layer recurrent neural network (RNN) is performed afterwards in order to capture high-order temporal information. 
Finally, a new label map is rendered again based on the corrected pose, which is feed into a segment CNN combining with the image.
In order to perform the experiments,  we built a dataset with a semantic labeled real world 3D map, jointly with many recorded videos with ground truth pose from high accurate motion sensors. We show that firstly, in a relative large environment, unlike the PoseNet~\cite{}, it is important to have a reference coarse pose signal. In addition, semantic information and pose are mutually beneficial in learning more robust networks. Finally, various ablation studies are performed, which demonstrate the effectiveness of the proposed system.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:introduction}
% the problem we are solving, distinguish with previous problems
In the applications like car navigation, due to the inaccuracy from motion sensors from mobile GPS and IMU, outdoor visual-based 6-DOF camera pose estimation and scene parsing~\cite{} are attracting much attention in the community of computer vision. 
Additionally, to acquire better scene understanding for applications like augment reality navigation~\cite{}, parsing each frame of a video is also important.

% existing methods only consider one of the tasks with soly visual signal. 
Currently, most existing vision algorithms are trying to solve both of the tasks solely depend on visual signals. 
For instances, in camera localization, geometric based methods are relying visual feature tracking, \eg systems of SLAM~\cite{}, PTAM~\cite{} and 
DTAM~\cite{}, where the environment point cloud is reconstructed during the localization. Such systems could be confused of the environment scale and also can not work when staic videos or a single image is provided.
To handle such senarios, visual feature matching or point cloud matching are oftenly used, \eg system of Perspective-n-Points (PnP)~\cite{}, or patch matching~\cite{}. 
Most recently, deep learning based methods, \eg either for images~\cite{} or videos~\cite{}, are also proposed for real-time localization, which show superior accuracy and speed tradeoff.
Nevertheless, single view methods are especially for specified feature rich landmarks, \eg, Cambridge buidings~\cite{}, while could easily fail for street views with very similar appearances.

For scene parsing, approaches~\cite{} based on deep fully convolutional network (FCN)~\cite{} and ResNet~\cite{} are the best algorithms for either single image or multiple frames according to Dataset like CityScape~\cite{}. For video inputs, recent works~\cite{} also try to incorporate the optical flow between consecutive frames, in order to accelerate the parsing and build the consistency along the temperal dimension.
However, since the camera is recording the same 3D scene, higher order relationship beyond nearby frame should also be considered.
In order to jointly consider parsing and 3D, prior arts~\cite{} before deep-learning try to do it typically by first do the reconstruction with structure-from-motion (SFM)~\cite{} and then parsing the images, 
 whereas the accuracy of reconstructed 3D from street-view images is limited for real applications.

In our scenario, targeting at a more practical system setting, in addition to online recorded video, for handling the localization confusion of street views, we propose to fuse the coarse GPS and IMU motion signal from corresponding device, which is typically available for current navigation system. Those signals can serve as a pose priori for our deep learning system. 
For video segmentation, in order to have 3D relationships between different frames, we pre-build a high resolution semantic 3D map based on high accurate LIDAR points recorded by Riegl~\cite{}, which helps construct the correlation in-between multiple frames through the estimated poses. 
In fact, our setting is realistic when compare with the widely applied commercial navigation system, such as Google Map~\cite{}, we raise the 2D road map to a 3D world, and try to online estimate the 3D camera pose rather than 2D. For 3D map, city scale 3D map has largely already collected from comany like Google Earth~\cite{} and Altzure~\cite{}, and also semantic labeled ones is also built such as Toronto city~\cite{}.

Finally, within such a framework, the pose estimation and scene parsing could be jointly modeled, where pose can help build the correlation between different frames, and reversely, semantics could help align camera poses, yielding better results for both tasks than doing them individually. In our experiments, our system estimates the pose in 10ms with accuracy under 1 degree, and segments the image $512 \times 608$ in less than 100ms with pixel accuracy around 96$\%$, which demonstrates its efficiency and effectiveness.

\begin{figure*}[t]
\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
   \caption{Framework of our proposed system. The black arrows show the testing process and red arrows show the back-propagation in training.}
\label{fig:framework}
\end{figure*}

% redefine our problem
In summary, the contributions of this paper are in three folds:
\begin{itemize}
    \item We propose a deep learning based method for fusing multiple sensors, \ie camera, GPS and IMU, which significantly improves the accuracy of awared visual tasks, \ie camera localization and scene parsing.
    \item We build a system that jointly do 6-DOF camera pose estimate and semantic parsing based on a deep learning strategy, where two kinds of information are mutually beneficial inside.
    \item To perform the experiments, we apply our system to a dataset recorded from real scene with dense 3D point cloud, ground truth camera poses and pixel-level parsing of every frame, which helps to evaluate the system and hopefully benefit the correlated research field in computer vision.
\end{itemize}

The structure of this paper is organized as follows. In \secref{sec:data_collection}, we first describe how our data is different from the existing outdoor datasets. In addition, we briefly introduce the method how the data is collect and labelled. Then, \secref{sec:localize_and_parsing} presents the framework and detail of our system. The full performance is evaluated quantitatively for both pose estimation and parsing in \secref{sec:experiments}, and \secref{sec:conclusion} concludes the paper and point out future directions. Finally, We will release all our code, models and dataset with the publication of this paper.


\subsection{Framework}
\label{sub:framework}
The framework of our system is illustrated in \figref{fig:framework}. At above, a pre-built 3D semantic map is prepared. During testing, an online stream of images and corresponding camera poses are fed in to the system. Firstly, for each frame, a semantic label map is rendered out based on the obtained coarse camera pose. Then it is fed to a pose network jointly with the respective image.  The network calculates their relative rotation and translation, and yields a corrected camera pose. To build the temporal correlations, the corrected poses are fed into a RNN which further improve the pose accuracy in the stream. 
Last, given the rectified camera pose, a new rendered label map is generated. We feed it together with the image to a segmentation network, which helps to segment a spatially more accurate and temporally more consistent result for the stream of video. 
In this system, since our data contains ground truth for both pose and segments, it can be trained with strong supervision at each end of outputs.

\section{Related work}
\label{sec:related_work}
Estimating camera pose and parsing images with a video or a single image have long been center problems for computer vision and robotics. 
Here we summarize the correlated works in several aspect without enumerating all works in this field due to space limitation.
\textbf{Structure from motion and PnP.}
\textbf{Joint 2D-3D for reconstruction and parsing.}
\textbf{Fusing sensors from GPS and camera.}
\textbf{Deep learning for camera pose and scene parsing.}
% traditional key points, slam etc
% pnp
% marc's paper
% hongdong's paper
% silvio's paper
% posenet related
% iccv kalman 
% domain transfer etc


\section{Constructing the data}
\label{sec:data_collection}

\begin{table*}[t]
\center
\begin{tabular}{lccccc}
\toprule[0.2 em]
% \thickhline
Dataset & Real Data & Camera pose & 3D map & Video per-frame labeling  & Temporal variations  \\ 
\hline 
\multicolumn{1}{l|}{CamVid~\cite{}}     &\checkmark                       & -              & -              &  -  & -  \\
\multicolumn{1}{l|}{KITTI~\cite{}}      &\checkmark  & \checkmark     & sparse points  & -   & -  \\
\multicolumn{1}{l|}{CityScapes~\cite{}} &\checkmark  & -              &  -             & selected frames & - \\
\multicolumn{1}{l|}{Toronto~\cite{}}    &\checkmark  & \checkmark     & sparse points  & selected pixels & - \\
\hline
\multicolumn{1}{l|}{Synthia~\cite{}}    & -          & \checkmark     & -       &\checkmark & -    \\ 
\multicolumn{1}{l|}{Play for benchmark~\cite{}} &-   & \checkmark     & -     &\checkmark & - \\
\hline 
\multicolumn{1}{l|}{Ours}              & Real        &\checkmark    &dense point cloud  & \checkmark     &  \checkmark \\
\toprule[0.2 em]
\end{tabular}
\caption{Compare our data with the other related outdoor street-view datasets for our task. 'Real Data' mean whether the data is collected from realistic world. 
'3D map' means whether it contains 3D map of the whole dataset. 'Video per-frame labeling' means whether it has per-frame per-pixel semantic label. 
'Temporal variations' mean whether the recorded video can roughly cover the whole scene, but have multiple.}
\label{tbl:data}
\vspace{-0.3\baselineskip}
\end{table*}

\paragraph{Motivation.}
As described in the \secref{sub:framework}, our system is performed with available motion sensors and a semantic 3D map. 
However, similar outdoor dataset produced by prior arts such as KITTI and CityScapes, does not containing such information. As summarized in \tabref{tbl:data}, we list several key properties to perform our experiments. 
As can be seen, existing ones do not satisfy our requirements, especially for pose estimation, for training, we want the recorded video to roughly cover the 3D environment, so that when new images come in, appearance similar views had been seen during training. This means repeated recording at similar spatial locations are required in the data, which we call temporal variations in \tabref{tbl:data}.
However, most existing datasets are a temporal snapshot at some locations in the environment, which require us to collect a new dataset ourselves.

\paragraph{Data collection.}
We use a high resolution LIDAR device called $Riegl$\footnote{http://www.rieglusa.com/index.html} to collect the 3D points, it is a time-of-flight laser radar with multiple wavelengths. 
However, since the turning rate of the , moving object inside scene will not be captured 
% talk about registration and removing moving objects inside data

% Undistortion of the image

\paragraph{Data labelling.}
% talk about labelling in 3D
Given the 3D data points, we first segment the data in to multiple clusters, and 

% talk about fixing in 2D

In \figref{},  we visualize a snapshot of our dataset, and we recommend to check the labelled videos from our supplementary material for a better sense of the dataset.



\begin{figure}[t]
\begin{center}
\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
\end{center}
   \caption{Snapshot of our collected data. Compare the rendered results with and without point enlargement handled by \equref{eq:label}.}
\label{fig:data}
\end{figure}

\section{Localizing camera and scene parsing.}
\label{sec:localize_and_parsing}
As shown in \secref{sub:framework}, our full system is based on a semantic 3D map and deep CNN. In the following, we will first describe how a semantic label map is rendered from the 3D, then talk about the details of our network architecture and the loss functionals to train the system.

\paragraph{Render a label map from a camera pose.} 
Formally, given a 6-DOF camera pose $\ve{p} = [\ve{q}, \ve{t}]$, where $\ve{q}$ is the quaternion representation of rotation and $\ve{t}$ is translation, a label map can be rendered from the semantic 3D map, where z-buffer~\cite{} is applied to find the closest point at each pixel.

In our setting, the 3D map is a point cloud based environment which includes 200M points. Although the density of the point cloud is very high (one point per 2.5 centimeters within road regions), when the 3D points are far away from the camera, the projected points could be very sparse, \eg regions of buildings etc.
Thus for each point in the environment, we enlarge the 3D point to a square piece where the square size is dependent on its semantic class. Formally, for a 3D point $\ve{x}$ belonging a class $c$, the square size $s_c$ is set to be,
\begin{align}
\label{eq:square_size}
s_c \propto \frac{1}{|\hua{P}_c|}\sum_{\ve{x}\in \hua{P}_c} \min_{\ve{t}\in\hua{T}} d(\ve{x}, \ve{t})
\end{align}
where $\hua{P}_c$ is the set of 3D points belong to class $c$, and $\hua{T}$ is the set of camera locations. Then, given the proportion, we rescale the square size within a range between a minimum size and a maximum size, which is set to be $[0.025, 0.05]$.

With such a strategy, as shown by \figref{fig:data}, regions with invalid value in-between projected points are in-painted, meanwhile the the boundaries between different segments are also kept. By feeding in such a rendered map, it facilitates the CNN to better discover scene layouts both for pose estimation and scene parsing.
One may wonder why we render semantic label map rather than synthesizing an RGB image for later inputs. 
This is because intuitively label maps is invariant to color changes, which yields better boundaries reflecting the scene layout. 

\subsection{Camera Localization with motion prior}
\paragraph{Pose rectify with road prior.} 

\begin{figure}[t]
\begin{center}
\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
\end{center}
   \caption{The GRU RNN network architecture for modeling a sequence of camera poses.}
\label{fig:rnn}
\end{figure}

\paragraph{CNN-GRU pose network architecture.} 
As shown in \figref{fig:framework}, the CNN of our pose network takes as inputs an image $\ve{I}$ and the rendered label map $\ve{L}^c$ from corresponding coarse camera pose $\ve{p}_i^c$. It outputs a 7 dimension vector $\hat{\ve{p}}_i$ representing the relative pose between the image and rendered label map, and the estimated pose respect the world is $\ve{p}_i = \ve{p}_i^c + \hat{\ve{p}_i}$.
For designing the network, in order to have large kernel to obtain bigger context while keeping the amount of parameters and runtime manageable, we follow the design of Ummenhofer \etal~\cite{}. The convolutional kernel of this network consists a pair of 1D filters in $y$ and $x$-direction, and the encoder gradually reduces the spatial resolution with stride of 2 while increasing the number of channels. We list the details of the network in our implementation details at \secref{sec:experiments}.

Additionally, after the output pose $\ve{p}_i$ from our pose network, as illustrated in \figref{fig:rnn}, a forward GRU-RNN is performed to model the temporal motion sequence of the video, since the movement of a vehicle is commonly smooth.
Traditionally in navigation applications of estimating 2D poses,  Kalman filter is applied by calculating either constant velocity or acceleration~\cite{}. 
In our case, transition of camera poses is learned from the training sequences, where the current motion is predicted by the RNN from a sequence of previous predicted poses, which yields further improvement over the estimated poses from the pose CNN.


\paragraph{Pose loss.} 
Following the PoseNet~\cite{}, we use the geometric matching loss for training, which avoids the balancing factor between rotation and translation. 
Formally, given a set of point cloud in 3D $~\hua{P}=\{\ve{x}\}$, and the loss for each image is written as,
\begin{align}
L(\ve{p}, \ve{p}^*) = \sum_{\ve{x} \in \hua{P}}\omega_{l_\ve{x}}|\pi(\ve{x}, \ve{p}) - \pi(\ve{x}, \ve{p}^*)|_2
\label{eq:proj_loss}
\end{align}
where $\ve{p}$ and $\ve{p}^*$ are the estimated pose and ground truth pose respectively. $\pi()$ is a projective function that maps a 3D point $\ve{x}$ to 2D image coordinates. $l_\ve{x}$ is the semantic label of $\ve{x}$ and $\omega_{l_\ve{x}}$ is a weight factor depended on the semantics. Here, we set stronger weights for point cloud belong to certain classes like traffic light, and we found it gives better performance for estimated poses.
In ~\cite{}, usually the 3D points that is visible by the camera are used which helps the stableness of learning. In our case, online searching the visible ones in millions of points are infeasible. 
Thus, we pre-render a depth map for each training image with resolution of $256 \times 304$, using the ground truth pose, and select those points projected on the depth map for training.

% Intuitively, we find the amount of points in each class is dramatically unbalanced, and most points are those on roads and trees. Nevertheless, the appearance variation of road and trees are not very sensitive to pose changes in the street-view scenario. In order to encourage the network to discover more str
% Thus,  other structures images like electricity pole or road light have rich structures like edges and textures, which potentially should be valued more for matching.
% In~\cite{ummenhofer2016demon}, the model try to predict a flow confidence map revealing the texture rich regions which helps pose estimation. In our work, we adopt the labelled semantic labels, and reweight each 3D point in the training loss, which drives the network to focus more on 
% Thus, the learning loss changed to 
% \begin{align}
% L(\ve{p}, \ve{p}^*) = \sum_{\ve{x} \in \hua{P}}\omega_{l_\ve{x}}|\pi(\ve{x}, \ve{p}) - \pi(\ve{x}, \ve{p}^*)|_2
% \label{eq:proj_loss}
% \end{align}
% where $\omega_{l_\ve{x}}$ is the weight of class $l_\ve{x}$. We set the weight for each class depends on the class edgeness which is the percentage of pixel along the edge 
\subsection{Video parsing with pose guidance}
Having rectified pose at hand, one may direct render the semantic 3D world to current view of a camera, yielding a semantic parsing of the current image. However, the estimated pose is not perfect, thus alignment of very think regions like light pole can totally fail. Another fact is that many LIDAR points are missing due to reflection, \eg regions of building mirrors, and points can be sparse at long distance. Finally, non-linear distortion of images and 3D map could happen due to system error, yielding scene miss alignment. Thus, we propose to a parameter light segment network to handle these issues. 
In our experiments, we show with pose rendered label map as an additional input, the segment results are temporally more consistent and yields better accuracy.

\begin{figure*}[]
\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
   \caption{Architecture of the segment network.}
\label{fig:segnet}
\end{figure*}
\paragraph{Segment network architecture.} As illustrated in \figref{fig:segnet}, the segment network contains an encoder-decoder network and a refinement network, and both have similar architecture with the corresponding ones used in DeMoN~\cite{ummenhofer2016demon}. 
The difference for encoder-decoder network is that for network inputs, rather than directly concatenate the label map with input image, to balance the two, we first transform the label map to a score map through one-hot operation, and embed the score of each pixel to a 32 dimensional feature vector. Then, we concatenate this feature vector with output features from those of the first layer, and keeps the rest as the same with that in~\cite{ummenhofer2016demon}. For refinement network, we use the same strategy to handle the two inputs. Finally, the segment network produce a score map, yielding the semantic label of each pixel.

We train the segment network firstly with only RGB images, then fine-tune the network by adding the input of rendered label maps. This is because our network is trained from scratch, thus needs large amounts of data to learn effective features from the image. However, in our experiments, the rendered label map from the estimated pose has on average 82$\%$ pixel accuracy. This could easily drive the network fitting the input label map at first, while slow down the process towards learning features from images. Finally, for segmentation loss, we use the standard softmax loss, and add intermediate supervision right after the output from encoder and decoder as indicated in \figref{fig:segnet}.


\section{Experiments}
\label{sec:experiments}
We perform all our experiments using our collected dataset, and evaluate multiple system settings for pose and segment to validate each component. 
For obtaining GPS and IMU signal, due to at each time step, one sampled noisy signal can obtained. This is limited for training the system. Thus, follow~\cite{}, we simulate GPS and IMU error by adding random noise $\epsilon$ with uniform distribution. Specifically, translation and rotation noise are set as $\epsilon_t \sim U(0, 7.5)$ and rotation $\epsilon_r \sim U(0^{\circ}, 15^{\circ})$ respectively. We refer to realistic data~\cite{} for setting range of simulation noise.

In this paper, our data vehicle collected 6 videos at different daytimes surrounding a technology park. The 3D map generated in our experiment has a road length around 3500$m$, and the distance between consecutive frames is around 5$m$ to 10$m$. We use 4 of the videos for training and 2 for testing, yielding 2242 training images and 756 testing images. The semantic classes included in the dataset includes $\{$sky, car-lane, ped-lane, bike-lane, curb, traffic-cone, traffic-stack, traffic-fence, light-pole, traffic-light, tele-pole, traffic-sign, billboard, building, security-stand, plants, object $\}$. In the future, we are targeting at constructing even larger datasets with more semantics.

\paragraph{Implementation details.} To quickly render from the 3D map, we adopt OpenGL to efficiently render a label map while handling the z-buffer operation. A 512 $\times$ 608 image can be generated in 70ms with a single Titan Z GPU, which is also the input size for both pose network and segment network. For segment output, we keep the size the same as input. Both of the network is learned with 'Nadam' optimizer~\cite{} with a learning rate $10^{-3}$. For training pose RNN, we sample a video sequence with 100 frames from the training data. We sequentially train the three models due to GPU memory limitation.
For pose CNN and segment CNN, we stops at 100 epochs, and for pose RNN, we stops at 200 epochs. For data augmentation, we use the imgaug~\footnote{} library to variate lighting, blurring and flipping. We select a subset from training images for validation trained model from each epoch, and choose the model performing best for evaluation.

For testing, to get a confidence range for the prediction for verifying the effectiveness of each model, we report the standard variation of the results from a 10 time simulation. Finally, we implement all the networks by adopting the MXNet~\cite{} platform. 


\paragraph{Pose Evaluation.}
In \tabref{tbl:pose}, we show the pose estimated results. We first direct following the work of PoseNet~\cite{kendall2017geometric}, and use their published code to train a model. Due to scene appearance similarity (\figref{fig:data}, it does not give reasonable results in our experiments. 
By using only CNN and coarse pose prior, we can start to learn reasonable relative poses (3rd row), which significantly reduces the pose error. When inserting semantic cues such as road priori and semantic weights in \equref{eq:proj_loss}, the pose results is significantly improved, especially at rotation (4th row). 
Finally, RNN gives strong cues about the moving speed and acceleration of the camera, yields the best results for both translation and rotation. 

\begin{table}
\center
\small
\begin{tabular}{lccc}
\toprule[0.1 em]
% \thickhline
Method & Trans (m) & Rot ($\circ$) & Pix. Acc($\%$)\\ 
\hline 
PoseNet~\cite{kendall2017geometric} & -  & -  & -  \\
Coarse pose & 3.35 $\pm$ 2.13 & 7.76 $\pm$ 4.14 & 54.01 $\pm$ 13.04 \\
Pose CNN w/o semantic & -  & -  & - \\
Pose CNN w semantic & -  & -  &  -  \\
Pose RNN w/o CNN & -  & -  &  -  \\
Pose CNN-RNN     &  &   &  \\
\toprule[0.1 em]
\end{tabular}
\caption{Compare the accuracy of different pose networks setting. $\pm$ indicates the confidence region by 10 time simulation.}
\label{tbl:pose}
\vspace{-0.3\baselineskip}
\end{table}

\paragraph{Segment Evaluation.}
In \tabref{tbl:segment}, we show the scene parsing results. We first use one of the state-of-the-art parsing network on the CityScapes \ie ResNet38~\cite{}. It has pre-trained parameters from both ImageNet and CityScapes, running with a 5s per-frame with our resolution. Although the results could be good, it is too slow for real-applications. Our network can run in 0.1s, while yields comparable results (4th row). At 2nd row, we use the ground truth pose to obtain an upper-bound for the segmentation performance. In this case, the projected label map aligns perfect with the image, thus yields significantly better results than the rest. At 3rd row, we train the segment network without pose prior.  Notice that for a fair comparison, after convergence at 100 epoch, we continue train the network another 100 epochs in order to avoid the influence from longer training. 
At 4th and 5th row, we show the results train with rendered label map from pose CNN and pose CNN-RNN respectively, where better pose can help the segment accuracy get closer to the oracle case, which demonstrate the effectiveness of our strategy. Due to space limit, we leave per-class results to the supplementary.

% \begin{table*}
% \center
% \begin{tabular}{lcccccccccccccccccc}
% \toprule[0.1 em]
% %\thickhline
% Method & mean IOU & sky & car-lane & ped-lane & bike-lane & curb & $t$-cone & $t$-stack & $t$-fence & light-pole & traffic-light & tele-pole & $t$-sign & ad-board & building & security-stand & plants & object \\ 
% \hline 
% ResNet38~\cite{} & & &  & & & & & & & & & & & & & & &  \\
% Segment CNN w oracle pose & & & & & & & & & & & & & & & & & &  \\
% Segment CNN w/o pose & & & & & & & & & & & & & & & & & &  \\
% Segment CNN w pose CNN & & & & & & & & & & & & & & & & & & \\
% Segment CNN w pose CNN-RNN & & & & & & & & & & & & & & & & & & \\
% \toprule[0.1 em]
% \end{tabular}
% \caption{Compare the accuracy of different segment networks setting. $t$ is short for 'traffic' in the table. $\pm$ indicates the confidence region by 10 time GPS simulation.}
% \label{tbl:segment}
% \vspace{-0.3\baselineskip}
% \end{table*}
\begin{table}[t]
\center
\small
\begin{tabular}{lccc}
\toprule[0.1 em]
%\thickhline
Method &  mIOU ($\%$) & mAcc ($\%$) & Pix. Acc ($\%$)\\
\hline 
ResNet38~\cite{} & & &  \\
SegCNN w/o Pose & & & \\
SegCNN w pose GT & & & \\
SegCNN w Pose CNN & & & \\
SegCNN Final & & & \\
\toprule[0.1 em]
\end{tabular}
\caption{Compare the accuracy of different segment networks setting. $\pm$ indicates the confidence region by 10 time GPS simulation.}
\label{tbl:segment}
\vspace{-0.3\baselineskip}
\end{table}

\paragraph{Qualitative results.} In \figref{fig:results}, we select a subsequence composed of 5 frames to show the results. More results are provided in the supplementary material. At 2nd and 3rd column, we show how the pose network aligns the image with pose from view of camera. At 4th and 5th column, we compare the segment results taking only images as input and those also consider rendered label map. As can be seen at the circled regions, the segment results with pose rendered label maps not only give better accuracy, but also have better temporal consistency.
\begin{figure*}[t]
\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
   \caption{Results from each intermediate stage out of the system.}
\label{fig:results}
\end{figure*}


\section{Conclusion}
\label{sec:conclusion}

\paragraph{Future work.} Since theoretically there exists optimal solutions which enforce the consistency between rendered label map from a camera pose and corresponding semantic segments, 
later we would like to design a real time solution for finding it for better localization.
In addition, we would like to model the surfaces which sketch out the 3D map, depend on our point cloud data, based on which a simpler and faster dense segment would performed.

% 
{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}

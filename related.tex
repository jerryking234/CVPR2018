\section{Related work}
\label{sec:related_work}
Estimating camera pose and parsing images with a video or a single image have long been center problems for computer vision and robotics.
Here we summarize the correlated works in several aspects without enumerating all works in relative field due to space limitation.
In our problem, we are interested in visual-based street-view cases, thus localization and general parsing in the wild is out of our scope.

% talk each perspective with image and video
\textbf{Camera pose estimation.} Traditionally, localizing an image given a set of 3D points is formulated as a Perspective-$n$-Point (P$n$P) problem~\cite{haralick1994review,kneip2014upnp} by matching feature points in 2D and features in 3D through cardinality maximization. Usually in a large environment, a pose prior is required in order to obtain good estimation~\cite{david2004softposit,moreno2008pose}. Campbell \etal~\cite{campbell2017globally} propose a global-optimal solver which leverage the prior. At the case that geo-tagged images are available, Sattler \etal~\cite{sattler2017large} propose to use image-retrieval to avoid matching large-scale point cloud.
When given a video, relative pose could be further modeled with methods like SLAM~\cite{engel2014lsd} etc, which increases the localization accuracy and speed.

Although these methods are effective in many cases with distinguished feature points, it is still not practical for city-scale environment with billions of points, and also may fail in low texture, thin structure and occlusions.
Thus, recently, deep learned features with CNN is proposed to combine low-level and high-level feature for localization. PoseNet~\cite{Kendall_2015_ICCV,kendall2017geometric} takes an low-resolution image as input, which can estimate pose in 10ms \wrt a feature rich environment composed of distinguished landmarks. LSTM-PoseNet~\cite{hazirbasimage} further captures a global context after CNN features.
Given an video, later works incorporate Bi-Directional LSTM~\cite{DBLP:journals/corr/ClarkWMTW17} or Kalman filter LSTM~\cite{coskun2017long} to obtain better results with temporal information. However, in street-view scenario, considering a road with trees aside, in most cases, no significant landmark appears, which could fail the visual models. Thus, although noisy, signals from GPS/IMU are a must assistant for localization in these cases~\cite{vishal2015accurate}, whereas the problem changes to estimating the relative pose between the camera view of a noisy pose and the real pose. To handle this, recently, Researchers ~\cite{laskar2017camera,ummenhofer2016demon} proposes to concatenate two real images as a network input, while in our case, we concatenate the real image with online rendered label map from the noisy pose, which provides superior results in our experiments.

\textbf{Scene parsing.} For parsing a single image, most state-of-the-arts (SOTA) algorithms over street-view datasets like CityScapes~\cite{Cordts2016Cityscapes} are designed based on a FCN~\cite{WuSH16e} and a multi-scale context module with either dilated convolution~\cite{ChenPSA17}, pooling~\cite{ZhaoSQWJ16}, CRF~\cite{} or spatial RNN~\cite{byeon2015scene}. However, they are dependent on a ResNet~\cite{HeZRS15} with hundreds of layers, which is comparably too heavy for real applications like augment reality. Some researchers apply small models~\cite{PaszkeCKC16} or model compression~\cite{ZhaoQSSJ17} for acceleration by sacrificing the accuracy.
When the input is a video, spatial-temporal graph is built, Kundu \etal~\cite{kundu2016feature} use 3D dense CRF to get temporal consistent results. Recently, optical flow~\cite{dosovitskiy2015flownet} between consecutive frames is computed to transfer label or features~\cite{gadde2017semantic,zhu2016deep} from previous frame to current.  Significant progress has been made, yet the consistency is built through flow, rather than 3D, and camera pose, which is a more compact information to use. In our case, we use the rendered label map from 3D as the prior to the segment network, which leverage the difficulty of scene parsing solely from image cue. For practicality, we also adopt the light weighted network from DeMoN~\cite{ummenhofer2016demon} to keep the system consistent.


\textbf{Joint 2D-3D for video parsing.} Another set of works that could be related with ours is joint reconstruction, pose estimation and parsing~\cite{kundu2014joint,hane2013joint} through 2D-3D consistency supervised training.
 Traditionally, those methods are reliance on structure-from-motion (SFM)~\cite{hane2013joint} from feature or photometric matching. Specifically, they reconstruct a 3D map and performing semantic parsing over 2D and 3D jointly, yielding consistent segmentation between multiple frames.
 Most recently, CNN-SLAM~\cite{tateno2017cnn} replace the 3D reconstruction using a depth network from a single image, and a segment network for image parsing.
 However, all there approaches is processed offline and only for static background, which does not satisfy our online setting. Moreover, the quality of reconstructed 3D is not comparable with the one directly collected from real world.
% traditional key points, slam etc
% pnp
% marc's paper
% hongdong's paper
% silvio's paper
% posenet related
% jifeng's paper
% iccv kalman
% domain transfer etc
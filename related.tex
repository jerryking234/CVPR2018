\vspace{-0.6\baselineskip}
\section{Related Work}
\vspace{-0.5\baselineskip}
\label{sec:related_work}
Estimating camera pose and semantic parsing given a video or a single image have long been center problems for computer vision.
Here we summarize the related works in several aspects without enumerating them all due to space limitation.
Notice that our application is autonomous driving and navigation, we therefore focus on outdoor cases with street-view input. 
Localization and general parsing in the wild is beyond the scope of this paper.

% talk each perspective with image and video
\textbf{Camera pose estimation.} Traditionally, localizing an image given a set of 3D points is formulated as a Perspective-$n$-Point (P$n$P) problem~\cite{haralick1994review,kneip2014upnp} by matching feature points in 2D and features in 3D through cardinality maximization. Usually in a large environment, a pose prior is required in order to obtain good estimation~\cite{david2004softposit,moreno2008pose}. Campbell \etal~\cite{campbell2017globally} propose a global-optimal solver which leverage the prior. In the case that geo-tagged images are available, Sattler \etal~\cite{sattler2017large} propose to use image-retrieval to avoid matching large-scale point cloud.
When given a video, temporal information could be further modeled with methods like SLAM~\cite{engel2014lsd} etc, which increases the localization accuracy and speed.

Although these methods are effective in cases with distinguished feature points, they are still not practical for city-scale environment with billions of points, and they may also fail in areas with low texture, repeated structures, and occlusions.
Thus, recently, deep learned features with hierarchical representations are proposed for localization. PoseNet~\cite{Kendall_2015_ICCV,kendall2017geometric} takes a low-resolution image as input, which can estimate pose in 10ms \wrt a feature rich environment composed of distinguished landmarks. LSTM-PoseNet~\cite{hazirbasimage} further captures a global spatial context after CNN features.
Given an video, later works incorporate Bi-Directional LSTM~\cite{DBLP:journals/corr/ClarkWMTW17} or Kalman filter LSTM~\cite{coskun2017long} to obtain better results with temporal information. However, in street-view scenario, considering a road with trees aside, in most cases, no significant landmark appears, which could fail the visual models. Thus, signals from GPS/IMU are a must-have for robust localization in these cases~\cite{vishal2015accurate}, whereas the problem switched to estimating the relative pose between the camera view from a noisy pose and the real pose. For finding relative camera pose of two views, recently, researchers ~\cite{laskar2017camera,ummenhofer2016demon} propose to stack the two images as a network input. In our case, we concatenate the real image with an online rendered label map from the noisy pose, which provides superior results in our experiments.

\textbf{Scene parsing.} For parsing a single image of street views (e.g., these from CityScapes~\cite{Cordts2016Cityscapes}), most state-of-the-arts (SOTA) algorithms are designed based on a FCN~\cite{WuSH16e} and a multi-scale context module with dilated convolution~\cite{ChenPSA17}, pooling~\cite{ZhaoSQWJ16}, CRF~\cite{higherordercrf_ECCV2016}, or spatial RNN~\cite{byeon2015scene}. However, they are dependent on a ResNet~\cite{HeZRS15} with hundreds of layers, which is too computationally expensive for applications that require real-time performance. 
Some researchers apply small models~\cite{PaszkeCKC16} or model compression~\cite{ZhaoQSSJ17} for acceleration, with the cost of reduced accuracy.
When the input is a video, spatial-temporal informations are jointly considered, Kundu \etal~\cite{kundu2016feature} use 3D dense CRF to get temporally consistent results. Recently, optical flow~\cite{dosovitskiy2015flownet} between consecutive frames is computed to transfer label or features~\cite{gadde2017semantic,zhu2016deep} from the previous frame to current one.  In our case, we connect consecutive video frames through 3D information and camera poses, which is a more compact representation for static background. 
In our case, we propose the projection from 3D maps as an additional input, which alleviates the difficulty of scene parsing solely from image cues. Additionally, we adopt a light weighted network from DeMoN~\cite{ummenhofer2016demon} for inference efficiency.%\textcolor[rgb]{1.00,0.00,0.00}{of what?}.

\textbf{Joint 2D-3D for video parsing.} Our work is also related to joint reconstruction, pose estimation and parsing~\cite{kundu2014joint,hane2013joint} through embedding 2D-3D consistency.
 Traditionally, reliant on structure-from-motion (SFM)~\cite{hane2013joint} from feature or photometric matching, those methods first reconstruct a 3D map, and then perform semantic parsing over 2D and 3D jointly, yielding geometrically consistent segmentation between multiple frames.
 Most recently, CNN-SLAM~\cite{tateno2017cnn} replaces traditional 3D reconstruction module with a single image depth network, and adopts a segment network for image parsing.
 However, all these approaches are processed off-line and only for static background, which do not satisfy our online setting. Moreover, the quality of a reconstructed 3D model is not comparable with the one collected with a 3D scanner. % \textcolor[rgb]{1.00,0.00,0.00}{you mean our scanner?}
% traditional key points, slam etc
% pnp
% marc's paper
% hongdong's paper
% silvio's paper
% posenet related
% jifeng's paper
% iccv kalman
% domain transfer etc

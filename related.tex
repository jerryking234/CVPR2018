\vspace{-0.6\baselineskip}
\section{Related Work}
\vspace{-0.5\baselineskip}
\label{sec:related_work}
Estimating camera pose and parsing images with a video or a single image have long been center problems for computer vision and robotics.
Here we summarize the related works in several aspects without enumerating them all due to space limitation.
Notice that our application is autonomous driving and navigation, we therefore focus on outdoor cases with street-level input. Localization and general parsing in the wild is beyond the scope of this paper.

% talk each perspective with image and video
\textbf{Camera pose estimation.} Traditionally, localizing an image given a set of 3D points is formulated as a Perspective-$n$-Point (P$n$P) problem~\cite{haralick1994review,kneip2014upnp} by matching feature points in 2D and features in 3D through cardinality maximization. Usually in a large environment, a pose prior is required in order to obtain good estimation~\cite{david2004softposit,moreno2008pose}. Campbell \etal~\cite{campbell2017globally} propose a global-optimal solver which leverage the prior. At the case that geo-tagged images are available, Sattler \etal~\cite{sattler2017large} propose to use image-retrieval to avoid matching large-scale point cloud.
When given a video, relative pose could be further modeled with methods like SLAM~\cite{engel2014lsd} etc, which increases the localization accuracy and speed.

Although these methods are effective in cases with distinguished feature points, they are still not practical for city-scale environment with billions of points, and they may also fail in areas with low texture, repeated structures, and occlusions.
Thus, recently, deep learned features with CNN is proposed to combine low-level and high-level feature for localization. PoseNet~\cite{Kendall_2015_ICCV,kendall2017geometric} takes an low-resolution image as input, which can estimate pose in 10ms \wrt a feature rich environment composed of distinguished landmarks. LSTM-PoseNet~\cite{hazirbasimage} further captures a global context after CNN features.
Given an video, later works incorporate Bi-Directional LSTM~\cite{DBLP:journals/corr/ClarkWMTW17} or Kalman filter LSTM~\cite{coskun2017long} to obtain better results with temporal information. However, in street-view scenario, considering a road with trees aside, in most cases, no significant landmark appears, which could fail the visual models. Thus, although noisy, signals from GPS/IMU are a must for localization in these cases~\cite{vishal2015accurate}, whereas the problem changes to estimating the relative pose between the camera view of a noisy pose and the real pose. For tackling relative pose of two images shotting at a same scene, recently, researchers ~\cite{laskar2017camera,ummenhofer2016demon} propose to concatenate the two images as a network input, while in our case, we concatenate the real image with online rendered label map from the noisy pose, which provides superior results in our experiments.

\textbf{Scene parsing.} For parsing a single image of street views (e.g., these from CityScapes~\cite{Cordts2016Cityscapes}), most state-of-the-arts (SOTA) algorithms are designed based on a FCN~\cite{WuSH16e} and a multi-scale context module with dilated convolution~\cite{ChenPSA17}, pooling~\cite{ZhaoSQWJ16}, CRF~\cite{higherordercrf_ECCV2016}, or spatial RNN~\cite{byeon2015scene}. However, they are dependent on a ResNet~\cite{HeZRS15} with hundreds of layers, which is too computationally expensive for applications that require real-time performance. 
Some researchers apply small models~\cite{PaszkeCKC16} or model compression~\cite{ZhaoQSSJ17} for acceleration, with the cost of reduced accuracy.
When the input is a video, spatial-temporal graph is built, Kundu \etal~\cite{kundu2016feature} use 3D dense CRF to get temporally consistent results. Recently, optical flow~\cite{dosovitskiy2015flownet} between consecutive frames is computed to transfer label or features~\cite{gadde2017semantic,zhu2016deep} from the previous frame to the current one.  Significant progress has been made, yet the consistency is built through flow, rather than 3D information and the camera pose, which are more compact for static background. In our case, we propose the projection from 3D as a prior, which alleviates the difficulty of scene parsing solely from image cues. For practicality, we adopt a light weighted network from DeMoN~\cite{ummenhofer2016demon} to keep the system fast in inference.%\textcolor[rgb]{1.00,0.00,0.00}{of what?}.

\textbf{Joint 2D-3D for video parsing.} Another set of works that could be related to ours is joint reconstruction, pose estimation and parsing~\cite{kundu2014joint,hane2013joint} through 2D-3D consistency supervised training.
 Traditionally, those methods are reliant on structure-from-motion (SFM)~\cite{hane2013joint} from feature or photometric matching. Specifically, they reconstruct a 3D map and perform semantic parsing over 2D and 3D jointly, yielding consistent segmentation between multiple frames.
 Most recently, CNN-SLAM~\cite{tateno2017cnn} replaces 43D reconstruction using a depth network from a single image, and a segment network for image parsing.
 However, all there approaches are processed off-line and only for static background, which does not satisfy our online setting. Moreover, the quality of a reeeeeereconstructed 3D model is not comparable with the one collected with a 3D scanner. % \textcolor[rgb]{1.00,0.00,0.00}{you mean our scanner?}
% traditional key points, slam etc
% pnp
% marc's paper
% hongdong's paper
% silvio's paper
% posenet related
% jifeng's paper
% iccv kalman
% domain transfer etc